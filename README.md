# ML-LHY

记录的是我在自学台湾大学李宏毅老师课程过程中整理的学习笔记与作业实现，主要作为后续学习大模型以及 AI Infra 相关内容的前置基础课程。

## 1 内容说明
主要包含以下两部分内容：
### 1.1 课程笔记（2021 版本）

系统整理了与 LLM 相关的前置基础知识，涵盖：
- 回归（Regression）
- 反向传播（Backpropagation）
- 神经网络优化技巧
- 分类（Classification）
- 卷积神经网络（CNN）
- 自注意力机制（Self-Attention）
- Transformer
- 自监督学习（BERT）
- 强化学习（Reinforcement Learning）
- 网络压缩（Network Compression）

### 1.2 作业实现（2023 版本）
作业部分基于更新后的课程设置，侧重于代码实现与实践理解，均使用 **PyTorch** 完成（作业实现并未以追求高分或复杂调参作为目标，而是用于验证和巩固对模型结构、算法流程以及实现细节的理解）

内容: 
- PyTorch Tutorial
- HW1：回归（Regression）
- HW2：分类（Classification）
- HW3：卷积神经网络（CNN）
- HW4：自注意力机制（Self-Attention）
- HW5：Transformer
- HW7：BERT
- HW12：强化学习（Reinforcement Learning）
- HW13：网络压缩（Network Compression）

作业评分: 

| 作业 | Private Score | Public Score |
|-----|--------------|---------------|
| HW1: Regression | 0.91 | 0.87 |
| HW2: Classification | 0.75 | 0.75 |
| HW3: CNN | 0.73 | 0.75 |
| HW4: Self-attention | 0.94 | 0.94 |
| HW5: Transformer | 26.78 | 26.78 |
| HW7: BERT | 0.76 | 0.75 |
| HW12: Reinforcement Learning | 289 | 289 |
| HW13: Network Compression | 0.72 | 0.72 |
