{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFEKWoh3p1Mv"
      },
      "source": [
        "# Homework Description\n",
        "- English to Chinese (Traditional) Translation\n",
        "  - Input: an English sentence         (e.g.\t\ttom is a student .)\n",
        "  - Output: the Chinese translation  (e.g. \t\t湯姆 是 個 學生 。)\n",
        "\n",
        "- TODO\n",
        "    - Train a simple RNN seq2seq to acheive translation\n",
        "    - Switch to transformer model to boost performance\n",
        "    - Apply Back-translation to furthur boost performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Vf1Q79XPQ3D",
        "outputId": "ad2e1ef8-140a-48f4-e088-212f58d8103d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jan  5 14:41:30 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-PCIE-40GB          On  |   00000000:3D:00.0 Off |                  Off |\n",
            "| N/A   39C    P0             37W /  250W |    6953MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   1  NVIDIA A100-PCIE-40GB          On  |   00000000:3E:00.0 Off |                  Off |\n",
            "| N/A   31C    P0             34W /  250W |       3MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   2  NVIDIA A100-PCIE-40GB          On  |   00000000:40:00.0 Off |                  Off |\n",
            "| N/A   30C    P0             33W /  250W |     115MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   3  NVIDIA A100-PCIE-40GB          On  |   00000000:41:00.0 Off |                  Off |\n",
            "| N/A   30C    P0             35W /  250W |       3MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   4  NVIDIA A100-PCIE-40GB          On  |   00000000:B1:00.0 Off |                  Off |\n",
            "| N/A   30C    P0             34W /  250W |       3MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   5  NVIDIA A100-PCIE-40GB          On  |   00000000:B2:00.0 Off |                  Off |\n",
            "| N/A   31C    P0             34W /  250W |       3MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   6  NVIDIA A100-PCIE-40GB          On  |   00000000:B4:00.0 Off |                  Off |\n",
            "| N/A   31C    P0             31W /  250W |       3MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   7  NVIDIA A100-PCIE-40GB          On  |   00000000:B5:00.0 Off |                  Off |\n",
            "| N/A   31C    P0             32W /  250W |       3MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A   1704665      C   ...eihao/miniconda3/envs/dl/bin/python       6850MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59neB_Sxp5Ub"
      },
      "source": [
        "# Download and import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rRlFbfFRpZYT",
        "outputId": "c2e805a7-0964-4191-82d0-3466b6d0c10e"
      },
      "outputs": [],
      "source": [
        "#!pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "#!pip install --upgrade jupyter ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fSksMTdmp-Wt",
        "outputId": "f54c747c-2b8f-4fb5-b87e-a293194d217d"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/pytorch/fairseq.git\n",
        "#!cd fairseq && git checkout 3f6ba43\n",
        "#!pip install --upgrade ./fairseq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uRLTiuIuqGNc",
        "outputId": "d23f6974-20ef-40eb-d0cb-11e8a0b48159"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/users/weihao/miniconda3/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2026-01-05 14:41:35 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n07Za1XqJzA"
      },
      "source": [
        "# Fix random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xllxxyWxqI7s"
      },
      "outputs": [],
      "source": [
        "seed = 33\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ORDJ-2qdYw"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "## En-Zh Bilingual Parallel Corpus\n",
        "* TED2020\n",
        "    - Raw: 400,726 (sentences)   \n",
        "    - Processed: 394,052 (sentences)\n",
        "    \n",
        "\n",
        "## Testdata\n",
        "- Size: 4,000 (sentences)\n",
        "- **Chinese translation is undisclosed. The provided (.zh) file is psuedo translation, each line is a '。'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQw2mY4Dqkzd"
      },
      "source": [
        "## Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SXT42xQtqijD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: This does not look like a tar archive\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: This does not look like a tar archive\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n",
            "mv: cannot stat '/home/users/weihao/dl/model_h5/DATA/rawdata/ted2020/raw.en': No such file or directory\n",
            "mv: cannot stat '/home/users/weihao/dl/model_h5/DATA/rawdata/ted2020/raw.zh': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "data_dir = './DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "urls = (\n",
        "    \"https://github.com/figisiwirf/ml2023-hw5-dataset/releases/download/v1.0.1/ml2023.hw5.data.tgz\",\n",
        "    \"https://github.com/figisiwirf/ml2023-hw5-dataset/releases/download/v1.0.1/ml2023.hw5.test.tgz\"\n",
        ")\n",
        "file_names = (\n",
        "    'ted2020.tgz', # train & dev\n",
        "    'test.tgz', # test\n",
        ")\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "prefix.mkdir(parents=True, exist_ok=True)\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = prefix/f\n",
        "    if not path.exists():\n",
        "        !wget {u} -O {path}\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "!mv {prefix/'test.en'} {prefix/'test.raw.en'}\n",
        "!mv {prefix/'test.zh'} {prefix/'test.raw.zh'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLkJwNiFrIwZ"
      },
      "source": [
        "## Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_uJYkCncrKJb"
      },
      "outputs": [],
      "source": [
        "src_lang = 'en'\n",
        "tgt_lang = 'zh'\n",
        "\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0t2CPt1brOT3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you so much, Chris.\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "And I say that sincerely, partly because I need that.\n",
            "Put yourselves in my position.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸。我非常感激。\n",
            "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
            "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
            "請你們設身處地為我想一想！\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.'+src_lang} -n 5\n",
        "!head {data_prefix+'.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRoE9UK7r1gY"
      },
      "source": [
        "## Preprocess files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3tzFwtnFrle3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def strQ2B(ustring):\n",
        "    \"\"\"Full width -> half width\"\"\"\n",
        "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # Full width space: direct conversion\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "\n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace('-', '') # remove '-'\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "    elif lang == 'zh':\n",
        "        s = strQ2B(s) # Q2B\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h_i8b1PRr9Nf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/users/weihao/dl/model_h5/DATA/rawdata/ted2020/train_dev.raw.clean.en & zh exists. skipping clean.\n",
            "/home/users/weihao/dl/model_h5/DATA/rawdata/ted2020/test.raw.clean.en & zh exists. skipping clean.\n"
          ]
        }
      ],
      "source": [
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gjT3XCy9r_rj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
        "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKb4u67-sT_Z"
      },
      "source": [
        "## Split into train/valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AuFKeDz3sGHL"
      },
      "outputs": [],
      "source": [
        "valid_ratio = 0.01 # 3000~4000 would suffice\n",
        "train_ratio = 1 - valid_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QR2NVldqsXyY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train/valid splits exists. skipping split.\n"
          ]
        }
      ],
      "source": [
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1rwQysTsdJq"
      },
      "source": [
        "## Subword Units\n",
        "Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.\n",
        "- We will use the [sentencepiece](#kudo-richardson-2018-sentencepiece) package\n",
        "- select 'unigram' or 'byte-pair encoding (BPE)' algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ecwllsa7sZRA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/users/weihao/dl/model_h5/DATA/rawdata/ted2020/spm8000.model exists. skipping spm_train.\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' works as well\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lQPRNldqse_V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/users/weihao/dl/model_h5/DATA/rawdata/ted2020/train.en exists. skipping spm_encode.\n",
            "/home/users/weihao/dl/model_h5/DATA/rawdata/ted2020/train.zh exists. skipping spm_encode.\n",
            "/home/users/weihao/dl/model_h5/DATA/rawdata/ted2020/valid.en exists. skipping spm_encode.\n",
            "/home/users/weihao/dl/model_h5/DATA/rawdata/ted2020/valid.zh exists. skipping spm_encode.\n"
          ]
        }
      ],
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4j6lXHjAsjXa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
            "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁opportunity ▁to ▁come ▁to ▁this ▁stage ▁ t wi ce ▁; ▁i ' m ▁extreme ly ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁ bl ow n ▁away ▁by ▁this ▁con f er ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
            "▁i ▁f le w ▁on ▁air ▁force ▁two ▁for ▁eight ▁years ▁.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
            "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
            "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對 我 之前 演講 的 好 評 ▁。\n",
            "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁!\n",
            "▁我 曾 搭 乘 副 總 統 專 機 八 年 ▁。\n"
          ]
        }
      ],
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59si_C0Wsms7"
      },
      "source": [
        "## Binarize the data with fairseq\n",
        "Prepare the files in pairs for both the source and target languages.\n",
        "\n",
        "In case a pair is unavailable, generate a pseudo pair to facilitate binarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w-cHVLSpsknh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA/data-bin/ted2020 exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref {prefix/'train'}\\\n",
        "        --validpref {prefix/'valid'}\\\n",
        "        --testpref {prefix/'test'}\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMuH1SWLPWA"
      },
      "source": [
        "# Configuration for experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5Luz3_tVLUxs"
      },
      "outputs": [],
      "source": [
        "config = Namespace(\n",
        "    datadir = \"./DATA/data-bin/ted2020\",\n",
        "    savedir = \"./checkpoints/rnn\",\n",
        "    source_lang = src_lang,\n",
        "    target_lang = tgt_lang,\n",
        "\n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,\n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=8192,\n",
        "    accum_steps=2,\n",
        "\n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "\n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "\n",
        "    # maximum epochs for training\n",
        "    max_epoch=30,\n",
        "    start_epoch=1,\n",
        "\n",
        "    # beam size for beam search\n",
        "    beam=5,\n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2,\n",
        "    max_len_b=20,\n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "\n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "\n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjrJFvyQLg86"
      },
      "source": [
        "# Logging\n",
        "- logging package logs ordinary messages\n",
        "- wandb logs the loss, bleu, etc. in the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-ZiMyDWALbDk"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNoSkK45Lmqc"
      },
      "source": [
        "# CUDA Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oqrsbmcoLqMl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 14:41:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2026-01-05 14:41:39 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.381 GB ; name = NVIDIA A100-PCIE-40GB                   \n",
            "2026-01-05 14:41:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
          ]
        }
      ],
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJuBIHLLt2D"
      },
      "source": [
        "# Dataloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpG4EBRLwe_"
      },
      "source": [
        "## We borrow the TranslationTask from fairseq\n",
        "* used to load the binarized data created above\n",
        "* well-implemented data iterator (dataloader)\n",
        "* built-in task.source_dictionary and task.target_dictionary are also handy\n",
        "* well-implemented beach search decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3gSEy1uFLvVs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 14:41:39 | INFO | fairseq.tasks.translation | [en] dictionary: 7992 types\n",
            "2026-01-05 14:41:39 | INFO | fairseq.tasks.translation | [zh] dictionary: 7992 types\n"
          ]
        }
      ],
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mR7Bhov7L4IU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 14:41:39 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2026-01-05 14:41:39 | INFO | fairseq.data.data_utils | loaded 390,112 examples from: ./DATA/data-bin/ted2020/train.en-zh.en\n",
            "2026-01-05 14:41:39 | INFO | fairseq.data.data_utils | loaded 390,112 examples from: ./DATA/data-bin/ted2020/train.en-zh.zh\n",
            "2026-01-05 14:41:39 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train en-zh 390112 examples\n",
            "2026-01-05 14:41:39 | INFO | fairseq.data.data_utils | loaded 3,940 examples from: ./DATA/data-bin/ted2020/valid.en-zh.en\n",
            "2026-01-05 14:41:39 | INFO | fairseq.data.data_utils | loaded 3,940 examples from: ./DATA/data-bin/ted2020/valid.en-zh.zh\n",
            "2026-01-05 14:41:39 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid en-zh 3940 examples\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P0BCEm_9L6ig"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  37,   19,   15,   78,  308, 1231,   63,   13,  453,   16,    9,  633,\n",
            "        1405, 2506,    7,   46,   65,  241,   20,  854,   11, 2032,   38,   11,\n",
            "         821,  140,   11,    5,  543,  178,  150,   38,   11,  138,   78,  584,\n",
            "         111,   12, 2368,  998, 2705, 1405,    6,    4,   96,   19,  798,  306,\n",
            "           9,   13,   23,  324,    6,   79,   38,    5, 2943,    6,   12,    5,\n",
            "          33,   51,   23,  173,   60,   23,   15,    6,    4,   11,   95, 2093,\n",
            "           6,   83,   54,    4,   11,  138,  654,  202,    4,   11, 2200, 1774,\n",
            "           4,   11,   37,  173,  106,  337,    5,  843,  142,    4,   11,   46,\n",
            "          15,   24,  641,   21,   18,   20,   15,    6,  512, 1657,    7,    2]),\n",
            " 'target': tensor([  39, 2039,  492,  667, 1601,  918,   77, 2927, 1618,   10,  194,  262,\n",
            "        1337, 1292, 1586, 2003,    4,    5, 1437,   44, 2739,   66,  237,  448,\n",
            "          66,  709,  275,   66,   44, 1284,  448,  669,  542, 1096,    4,    5,\n",
            "         846,  526,   45,   41, 2809, 1417, 2777,  380, 3101, 4237, 3529, 4356,\n",
            "         393,   62,    8,  721, 3436,    4, 1157,  786,  393,  294, 1208,   66,\n",
            "        3627, 1296,  944,  557, 1601,  918,   66,    6,   73,  407, 1774, 1283,\n",
            "        1478,   66, 1216,  592, 3140, 1601,  918,  163,  785,  243,  390,  426,\n",
            "           4,  194,  439,  166, 2345, 1349,  371,  103,   77,   10,    2])}\n",
            "(\"Source: so i've been working with a lot of the big food companies . they can \"\n",
            " 'make it fun and sexy and hip and crunchy and convenient to eat healthier '\n",
            " \"foods , like i chair the advisory boards to mcdonald's , and pepsico , and \"\n",
            " \"conagra , and safeway , and soon del monte , and they're finding that it's \"\n",
            " 'good business .')\n",
            "('Target: 我嘗試跟許多食品大廠合作 。 他們讓吃健康食物這件事 , 變得有趣、性感、流行、有口感又方便 , 例如:我擔任麥當勞諮詢董事會的主席 , '\n",
            " '還有百事可樂、康尼格拉食品、safeway超市、台爾蒙食品也即將加入 , 他們發現這個市場商機很大 。')\n"
          ]
        }
      ],
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcfCVa2FMBSE"
      },
      "source": [
        "# Dataset iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBvc-B_6MKZM"
      },
      "source": [
        "* Controls every batch to contain no more than N tokens, which optimizes GPU memory efficiency\n",
        "* Shuffles the training set for every epoch\n",
        "* Ignore sentences exceeding maximum length\n",
        "* Pad all sentences in a batch to the same length, which enables parallel computing by GPU\n",
        "* Add eos and shift one token\n",
        "    - teacher forcing: to train the model to predict the next token based on prefix, we feed the right shifted target sequence as the decoder input.\n",
        "    - generally, prepending bos to the target would do the job (as shown below)\n",
        "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
        "    - in fairseq however, this is done by moving the eos token to the begining. Empirically, this has the same effect. For instance:\n",
        "    ```\n",
        "    # output target (target) and Decoder input (prev_output_tokens):\n",
        "                   eos = 2\n",
        "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
        "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OWFJFmCnMDXW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 14:41:39 | WARNING | fairseq.tasks.fairseq_task | 2,556 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[2654, 2291, 3666, 3032, 3527, 2653, 2468, 1745, 790, 3884]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': tensor([2620]),\n",
              " 'nsentences': 1,\n",
              " 'ntokens': 9,\n",
              " 'net_input': {'src_tokens': tensor([[   1,    1,    1,    1,    1,   57,   22,   75,  287,   12, 1658,  116,\n",
              "            139,  107,    7,    2]]),\n",
              "  'src_lengths': tensor([11]),\n",
              "  'prev_output_tokens': tensor([[   2, 2273,  762,  209,  165,   50, 2083, 1383,   10,    1,    1,    1,\n",
              "              1,    1,    1,    1]])},\n",
              " 'target': tensor([[2273,  762,  209,  165,   50, 2083, 1383,   10,    2,    1,    1,    1,\n",
              "             1,    1,    1,    1]])}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond\n",
        "        # first call of this method has no effect.\n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p86K-0g7Me4M"
      },
      "source": [
        "* each batch is a python dict, with string key and Tensor value. Contents are described below:\n",
        "```python\n",
        "batch = {\n",
        "    \"id\": id, # id for each example\n",
        "    \"nsentences\": len(samples), # batch size (sentences)\n",
        "    \"ntokens\": ntokens, # batch size (tokens)\n",
        "    \"net_input\": {\n",
        "        \"src_tokens\": src_tokens, # sequence in source language\n",
        "        \"src_lengths\": src_lengths, # sequence length of each example before padding\n",
        "        \"prev_output_tokens\": prev_output_tokens, # right shifted target, as mentioned above.\n",
        "    },\n",
        "    \"target\": target, # target sequence\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EyDBE5ZMkFZ"
      },
      "source": [
        "# Model Architecture\n",
        "* We again inherit fairseq's encoder, decoder and model, so that in the testing phase we can directly leverage fairseq's beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Hzh74qLIMfW_"
      },
      "outputs": [],
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder,\n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI46v1z7MotH"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn0wSeLLMrbc"
      },
      "source": [
        "- The Encoder is a RNN or Transformer Encoder. The following description is for RNN. For every input token, Encoder will generate a output vector and a hidden states vector, and the hidden states vector is passed on to the next step. In other words, the Encoder sequentially reads in the input sequence, and outputs a single vector at each timestep, then finally outputs the final hidden states, or content vector, at the last timestep.\n",
        "- Parameters:\n",
        "  - *args*\n",
        "      - encoder_embed_dim: the dimension of embeddings, this compresses the one-hot vector into fixed dimensions, which achieves dimension reduction\n",
        "      - encoder_ffn_embed_dim is the dimension of hidden states and output vectors\n",
        "      - encoder_layers is the number of layers for Encoder RNN\n",
        "      - dropout determines the probability of a neuron's activation being set to 0, in order to prevent overfitting. Generally this is applied in training, and removed in testing.\n",
        "  - *dictionary*: the dictionary provided by fairseq. it's used to obtain the padding index, and in turn the encoder padding mask.\n",
        "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
        "\n",
        "- Inputs:\n",
        "    - *src_tokens*: integer sequence representing english e.g. 1, 28, 29, 205, 2\n",
        "- Outputs:\n",
        "    - *outputs*: the output of RNN at each timestep, can be furthur processed by Attention\n",
        "    - *final_hiddens*: the hidden states of each timestep, will be passed to decoder for decoding\n",
        "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WcX3W4iGMq-S"
      },
      "outputs": [],
      "source": [
        "class RNNEncoder(FairseqEncoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "\n",
        "        self.embed_dim = args.encoder_embed_dim\n",
        "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
        "        self.num_layers = args.encoder_layers\n",
        "\n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim,\n",
        "            self.hidden_dim,\n",
        "            self.num_layers,\n",
        "            dropout=args.dropout,\n",
        "            batch_first=False,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "\n",
        "        self.padding_idx = dictionary.pad()\n",
        "\n",
        "    def combine_bidir(self, outs, bsz: int):\n",
        "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
        "        return out.view(self.num_layers, bsz, -1)\n",
        "\n",
        "    def forward(self, src_tokens, **unused):\n",
        "        bsz, seqlen = src_tokens.size()\n",
        "\n",
        "        # get embeddings\n",
        "        x = self.embed_tokens(src_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        # pass thru bidirectional RNN\n",
        "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
        "        x, final_hiddens = self.rnn(x, h0)\n",
        "        outputs = self.dropout_out_module(x)\n",
        "        # outputs = [sequence len, batch size, hid dim * directions]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "\n",
        "        # Since Encoder is bidirectional, we need to concatenate the hidden states of two directions\n",
        "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
        "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
        "\n",
        "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
        "        return tuple(\n",
        "            (\n",
        "                outputs,  # seq_len x batch x hidden\n",
        "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
        "                encoder_padding_mask,  # seq_len x batch\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def reorder_encoder_out(self, encoder_out, new_order):\n",
        "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
        "        return tuple(\n",
        "            (\n",
        "                encoder_out[0].index_select(1, new_order),\n",
        "                encoder_out[1].index_select(1, new_order),\n",
        "                encoder_out[2].index_select(1, new_order),\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZlE_1JnMv56"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSFSKt_ZMzgh"
      },
      "source": [
        "- When the input sequence is long, \"content vector\" alone cannot accurately represent the whole sequence, attention mechanism can provide the Decoder more information.\n",
        "- According to the **Decoder embeddings** of the current timestep, match the **Encoder outputs** with decoder embeddings to determine correlation, and then sum the Encoder outputs weighted by the correlation as the input to **Decoder** RNN.\n",
        "- Common attention implementations use neural network / dot product as the correlation between **query** (decoder embeddings) and **key** (Encoder outputs), followed by **softmax**  to obtain a distribution, and finally **values** (Encoder outputs) is **weighted sum**-ed by said distribution.\n",
        "\n",
        "- Parameters:\n",
        "  - *input_embed_dim*: dimensionality of key, should be that of the vector in decoder to attend others\n",
        "  - *source_embed_dim*: dimensionality of query, should be that of the vector to be attended to (encoder outputs)\n",
        "  - *output_embed_dim*: dimensionality of value, should be that of the vector after attention, expected by the next layer\n",
        "\n",
        "- Inputs:\n",
        "    - *inputs*: is the key, the vector to attend to others\n",
        "    - *encoder_outputs*:  is the query/value, the vector to be attended to\n",
        "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n",
        "- Outputs:\n",
        "    - *output*: the context vector after attention\n",
        "    - *attention score*: the attention distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1Atf_YuCMyyF"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
        "        self.output_proj = nn.Linear(\n",
        "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
        "        # inputs: T, B, dim\n",
        "        # encoder_outputs: S x B x dim\n",
        "        # padding mask:  S x B\n",
        "\n",
        "        # convert all to batch first\n",
        "        inputs = inputs.transpose(1,0) # B, T, dim\n",
        "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
        "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
        "\n",
        "        # project to the dimensionality of encoder_outputs\n",
        "        x = self.input_proj(inputs)\n",
        "\n",
        "        # compute attention\n",
        "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
        "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
        "\n",
        "        # cancel the attention at positions corresponding to padding\n",
        "        if encoder_padding_mask is not None:\n",
        "            # leveraging broadcast  B, S -> (B, 1, S)\n",
        "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
        "            attn_scores = (\n",
        "                attn_scores.float()\n",
        "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
        "                .type_as(attn_scores)\n",
        "            )  # FP16 support: cast to float and back\n",
        "\n",
        "        # softmax on the dimension corresponding to source sequence\n",
        "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
        "        x = torch.bmm(attn_scores, encoder_outputs)\n",
        "\n",
        "        # (B, T, dim)\n",
        "        x = torch.cat((x, inputs), dim=-1)\n",
        "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
        "\n",
        "        # restore shape (B, T, dim) -> (T, B, dim)\n",
        "        return x.transpose(1,0), attn_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doSCOA2gM7fK"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M8Vod2gNABR"
      },
      "source": [
        "* The hidden states of **Decoder** will be initialized by the final hidden states of **Encoder** (the content vector)\n",
        "* At the same time, **Decoder** will change its hidden states based on the input of the current timestep (the outputs of previous timesteps), and generates an output\n",
        "* Attention improves the performance\n",
        "* The seq2seq steps are implemented in decoder, so that later the Seq2Seq class can accept RNN and Transformer, without furthur modification.\n",
        "- Parameters:\n",
        "  - *args*\n",
        "      - decoder_embed_dim: is the dimensionality of the decoder embeddings, similar to encoder_embed_dim，\n",
        "      - decoder_ffn_embed_dim: is the dimensionality of the decoder RNN hidden states, similar to encoder_ffn_embed_dim\n",
        "      - decoder_layers: number of layers of RNN decoder\n",
        "      - share_decoder_input_output_embed: usually, the projection matrix of the decoder will share weights with the decoder input embeddings\n",
        "  - *dictionary*: the dictionary provided by fairseq\n",
        "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
        "- Inputs:\n",
        "    - *prev_output_tokens*: integer sequence representing the right-shifted target e.g. 1, 28, 29, 205, 2\n",
        "    - *encoder_out*: encoder's output.\n",
        "    - *incremental_state*: in order to speed up decoding during test time, we will save the hidden state of each timestep. see forward() for details.\n",
        "- Outputs:\n",
        "    - *outputs*: the logits (before softmax) output of decoder for each timesteps\n",
        "    - *extra*: unsused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QfvgqHYDM6Lp"
      },
      "outputs": [],
      "source": [
        "class RNNDecoder(FairseqIncrementalDecoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "\n",
        "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder\n",
        "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
        "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires\n",
        "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
        "\n",
        "        self.embed_dim = args.decoder_embed_dim\n",
        "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
        "        self.num_layers = args.decoder_layers\n",
        "\n",
        "\n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim,\n",
        "            self.hidden_dim,\n",
        "            self.num_layers,\n",
        "            dropout=args.dropout,\n",
        "            batch_first=False,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.attention = AttentionLayer(\n",
        "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
        "        )\n",
        "        # self.attention = None\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "\n",
        "        if self.hidden_dim != self.embed_dim:\n",
        "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
        "        else:\n",
        "            self.project_out_dim = None\n",
        "\n",
        "        if args.share_decoder_input_output_embed:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.embed_tokens.weight.shape[1],\n",
        "                self.embed_tokens.weight.shape[0],\n",
        "                bias=False,\n",
        "            )\n",
        "            self.output_projection.weight = self.embed_tokens.weight\n",
        "        else:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.output_embed_dim, len(dictionary), bias=False\n",
        "            )\n",
        "            nn.init.normal_(\n",
        "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
        "            )\n",
        "\n",
        "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
        "        # extract the outputs from encoder\n",
        "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
        "        # outputs:          seq_len x batch x num_directions*hidden\n",
        "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
        "        # padding_mask:     seq_len x batch\n",
        "\n",
        "        if incremental_state is not None and len(incremental_state) > 0:\n",
        "            # if the information from last timestep is retained, we can continue from there instead of starting from bos\n",
        "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
        "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        else:\n",
        "            # incremental state does not exist, either this is training time, or the first timestep of test time\n",
        "            # prepare for seq2seq: pass the encoder_hidden to the decoder hidden states\n",
        "            prev_hiddens = encoder_hiddens\n",
        "\n",
        "        bsz, seqlen = prev_output_tokens.size()\n",
        "\n",
        "        # embed tokens\n",
        "        x = self.embed_tokens(prev_output_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        # decoder-to-encoder attention\n",
        "        if self.attention is not None:\n",
        "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
        "\n",
        "        # pass thru unidirectional RNN\n",
        "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
        "        # outputs = [sequence len, batch size, hid dim]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        x = self.dropout_out_module(x)\n",
        "\n",
        "        # project to embedding size (if hidden differs from embed size, and share_embedding is True,\n",
        "        # we need to do an extra projection)\n",
        "        if self.project_out_dim != None:\n",
        "            x = self.project_out_dim(x)\n",
        "\n",
        "        # project to vocab size\n",
        "        x = self.output_projection(x)\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(1, 0)\n",
        "\n",
        "        # if incremental, record the hidden states of current timestep, which will be restored in the next timestep\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": final_hiddens,\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "\n",
        "        return x, None\n",
        "\n",
        "    def reorder_incremental_state(\n",
        "        self,\n",
        "        incremental_state,\n",
        "        new_order,\n",
        "    ):\n",
        "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
        "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDAPmxjRNEEL"
      },
      "source": [
        "## Seq2Seq\n",
        "- Composed of **Encoder** and **Decoder**\n",
        "- Recieves inputs and pass to **Encoder**\n",
        "- Pass the outputs from **Encoder** to **Decoder**\n",
        "- **Decoder** will decode according to outputs of previous timesteps as well as **Encoder** outputs  \n",
        "- Once done decoding, return the **Decoder** outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oRwKdLa0NEU6"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens,\n",
        "        src_lengths,\n",
        "        prev_output_tokens,\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens,\n",
        "            encoder_out=encoder_out,\n",
        "            src_lengths=src_lengths,\n",
        "            return_all_hiddens=return_all_hiddens,\n",
        "        )\n",
        "        return logits, extra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu3C2JfqNHzk"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nyI9FOx-NJ2m"
      },
      "outputs": [],
      "source": [
        "# # HINT: transformer architecture\n",
        "from fairseq.models.transformer import (\n",
        "    TransformerEncoder,\n",
        "    TransformerDecoder,\n",
        ")\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # token embeddings\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "\n",
        "    # encoder decoder\n",
        "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
        "    #encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    #decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "    # sequence to sequence model\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "\n",
        "    # initialization for seq2seq model is important, requires extra handling\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        if isinstance(module, MultiheadAttention):\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    # weight initialization\n",
        "    model.apply(init_params)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5n4eS7NQNy"
      },
      "source": [
        "## Architecture Related Configuration\n",
        "\n",
        "For strong baseline, please refer to the hyperparameters for *transformer-base* in Table 3 in [Attention is all you need](#vaswani2017)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Cyn30VoGNT6N"
      },
      "outputs": [],
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=512,\n",
        "    encoder_ffn_embed_dim=2048,\n",
        "    encoder_layers=6,\n",
        "    decoder_embed_dim=512,\n",
        "    decoder_ffn_embed_dim=2048,\n",
        "    decoder_layers=6,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.3,\n",
        ")\n",
        "\n",
        "# HINT: these patches on parameters for Transformer\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=8\n",
        "    args.encoder_normalize_before=True\n",
        "\n",
        "    args.decoder_attention_heads=8\n",
        "    args.decoder_normalize_before=True\n",
        "\n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "\n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Nbb76QLCNZZZ"
      },
      "outputs": [],
      "source": [
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(arch_args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7ZWfxsCDNatH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 14:41:41 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(7992, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(7992, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=512, out_features=7992, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHll7GRNNdqc"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUB9f1WCNgMH"
      },
      "source": [
        "## Loss: Label Smoothing Regularization\n",
        "* let the model learn to generate less concentrated distribution, and prevent over-confidence\n",
        "* sometimes the ground truth may not be the only answer. thus, when calculating loss, we reserve some probability for incorrect labels\n",
        "* avoids overfitting\n",
        "\n",
        "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "IgspdJn0NdYF"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, lprobs, target):\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        #  reserve some probability for other labels. thus when calculating cross-entropy,\n",
        "        # equivalent to summing the log probs of all labels\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index)\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1)\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce:\n",
        "            nll_loss = nll_loss.sum()\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "        # when calculating cross-entropy, add the loss of other labels\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "# generally, 0.1 is good enough\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRalDto2NkJJ"
      },
      "source": [
        "## Optimizer: Adam + lr scheduling\n",
        "Inverse square root scheduling is important to the stability when training Transformer. It's later used on RNN as well.\n",
        "Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.\n",
        "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sS7tQj1ROBYm"
      },
      "outputs": [],
      "source": [
        "def get_rate(d_model, step_num, warmup_step):\n",
        "    return (d_model ** -0.5) * min(\n",
        "        step_num ** -0.5,\n",
        "        step_num * warmup_step ** -1.5\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "J8hoAjHPNkh3"
      },
      "outputs": [],
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "\n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "\n",
        "    def multiply_grads(self, c):\n",
        "        \"\"\"Multiplies grads by a constant *c*.\"\"\"\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c)\n",
        "\n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFJlkOMONsc6"
      },
      "source": [
        "## Scheduling Visualized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "A135fwPCNrQs"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim,\n",
        "    factor=config.lr_factor,\n",
        "    warmup=config.lr_warmup,\n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOR0g-cVO5ZO"
      },
      "source": [
        "# Training Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0ZjbK3O8Iv"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "foal3xM1O404"
      },
      "outputs": [],
      "source": [
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
        "\n",
        "    stats = {\"loss\": []}\n",
        "    scaler = GradScaler() # automatic mixed precision (amp)\n",
        "\n",
        "    model.train()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
        "    for samples in progress:\n",
        "        model.zero_grad()\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "        # gradient accumulation: update every accum_steps samples\n",
        "        for i, sample in enumerate(samples):\n",
        "            if i == 1:\n",
        "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size_i = sample[\"ntokens\"]\n",
        "            sample_size += sample_size_i\n",
        "\n",
        "            # mixed precision training\n",
        "            with autocast():\n",
        "                net_output = model.forward(**sample[\"net_input\"])\n",
        "                lprobs = F.log_softmax(net_output[0], -1)\n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "\n",
        "                # logging\n",
        "                accum_loss += loss.item()\n",
        "                # back-prop\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "        scaler.unscale_(optimizer)\n",
        "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
        "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # logging\n",
        "        loss_print = accum_loss/sample_size\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        progress.set_postfix(loss=loss_print)\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss_print,\n",
        "                \"train/grad_norm\": gnorm.item(),\n",
        "                \"train/lr\": optimizer.rate(),\n",
        "                \"train/sample_size\": sample_size,\n",
        "            })\n",
        "\n",
        "    loss_print = np.mean(stats[\"loss\"])\n",
        "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1lX3DRO_yU"
      },
      "source": [
        "## Validation & Inference\n",
        "To prevent overfitting, validation is required every epoch to validate the performance on unseen data.\n",
        "- the procedure is essensially same as training, with the addition of inference step\n",
        "- after validation we can save the model weights\n",
        "\n",
        "Validation loss alone cannot describe the actual performance of the model\n",
        "- Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation\n",
        "- We can also manually examine the hypotheses' quality\n",
        "- We use fairseq's sequence generator for beam search to generate translation hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2og80HYQPAKq"
      },
      "outputs": [],
      "source": [
        "# fairseq's beam search generator\n",
        "# given model and input seqeunce, produce translation hypotheses by beam search\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # convert from Tensor to human readable sentence\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()),\n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()),\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "y1o7LeDkPDsd"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import sacrebleu\n",
        "\n",
        "def validate(model, task, criterion, log_to_wandb=True):\n",
        "    logger.info('begin validation')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "\n",
        "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "\n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "\n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "\n",
        "    if config.use_wandb and log_to_wandb:\n",
        "        wandb.log({\n",
        "            \"valid/loss\": stats[\"loss\"],\n",
        "            \"valid/bleu\": stats[\"bleu\"].score,\n",
        "        }, commit=False)\n",
        "\n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source: \" + srcs[showid])\n",
        "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
        "    logger.info(\"example reference: \" + refs[showid])\n",
        "\n",
        "    # show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRF6nd4PGEE"
      },
      "source": [
        "# Save and Load Model Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "edBuLlkuPGr9"
      },
      "outputs": [],
      "source": [
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):\n",
        "    stats = validate(model, task, criterion)\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        # save epoch checkpoints\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "\n",
        "        # save epoch samples\n",
        "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        # get best valid bleu\n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "\n",
        "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "        if del_file.exists():\n",
        "            del_file.unlink()\n",
        "\n",
        "    return stats\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir)/name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath)\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyIFpibfPJ5u"
      },
      "source": [
        "# Main\n",
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hu7RZbCUPKQr"
      },
      "outputs": [],
      "source": [
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "5xxlJxU2PeAo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 14:41:42 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2026-01-05 14:41:42 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2026-01-05 14:41:42 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2026-01-05 14:41:42 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2026-01-05 14:41:42 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2026-01-05 14:41:42 | INFO | hw5.seq2seq | num. model params: 71,226,368 (num. trained: 71,226,368)\n",
            "2026-01-05 14:41:42 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MSPRqpQUPfaX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 14:41:42 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326674]\n",
            "2026-01-05 14:41:42 | INFO | hw5.seq2seq | no checkpoints found at checkpoints/rnn/checkpoint_last.pt!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "next_epoch_idx = 1\n",
            "max_epoch = 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 14:41:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "/tmp/ipykernel_2185473/3159420106.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() # automatic mixed precision (amp)\n",
            "train epoch 1:   0%|          | 0/794 [00:00<?, ?it/s]/tmp/ipykernel_2185473/3159420106.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/home/users/weihao/miniconda3/envs/dl/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "2026-01-05 14:44:05 | INFO | hw5.seq2seq | training loss: 6.7055           \n",
            "2026-01-05 14:44:05 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 14:44:32 | INFO | hw5.seq2seq | example source: in germany , we have a system where you are not allowed to bribe a civil servant , but you are allowed to bribe a deputy .\n",
            "2026-01-05 14:44:32 | INFO | hw5.seq2seq | example hypothesis: 在美國 , 我們的研究中 , 不管我們不見 , 不管你不管 , 但你能讓它 。\n",
            "2026-01-05 14:44:32 | INFO | hw5.seq2seq | example reference: 在德國的系統裡 , 你不能夠賄賂一位公務員 , 但是可以賄賂一位代理人 。\n",
            "2026-01-05 14:44:32 | INFO | hw5.seq2seq | validation loss:\t5.4820\n",
            "2026-01-05 14:44:32 | INFO | hw5.seq2seq | BLEU = 2.71 22.7/5.6/1.6/0.5 (BP = 0.872 ratio = 0.880 hyp_len = 99154 ref_len = 112709)\n",
            "2026-01-05 14:44:32 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint1.pt\n",
            "2026-01-05 14:44:32 | INFO | hw5.seq2seq | end of epoch 1\n",
            "2026-01-05 14:44:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 14:46:57 | INFO | hw5.seq2seq | training loss: 5.1094           \n",
            "2026-01-05 14:46:57 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 14:47:24 | INFO | hw5.seq2seq | example source: but the challenge is trying to figure out which ones they are and why do we retain them , and do they have meaning .\n",
            "2026-01-05 14:47:24 | INFO | hw5.seq2seq | example hypothesis: 但挑戰是 , 要找出一個挑戰 , 為何他們為何處 , 為何他們有意義 , 為何他們有意義 。\n",
            "2026-01-05 14:47:24 | INFO | hw5.seq2seq | example reference: 但是 , 艱巨的任務是試著去找出哪些是應該保留 , 為什麼我們要保留它們 , 它們有什麼意義 。\n",
            "2026-01-05 14:47:24 | INFO | hw5.seq2seq | validation loss:\t4.5764\n",
            "2026-01-05 14:47:24 | INFO | hw5.seq2seq | BLEU = 10.11 35.7/14.7/6.5/3.1 (BP = 1.000 ratio = 1.088 hyp_len = 122664 ref_len = 112709)\n",
            "2026-01-05 14:47:24 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint2.pt\n",
            "2026-01-05 14:47:25 | INFO | hw5.seq2seq | end of epoch 2\n",
            "2026-01-05 14:47:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 14:49:47 | INFO | hw5.seq2seq | training loss: 4.4568           \n",
            "2026-01-05 14:49:47 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 14:50:10 | INFO | hw5.seq2seq | example source: you know how to immunize kids , you know how to give bed nets . \"\n",
            "2026-01-05 14:50:10 | INFO | hw5.seq2seq | example hypothesis: 你知道 , 要如何讓孩子受感染 。 」\n",
            "2026-01-05 14:50:10 | INFO | hw5.seq2seq | example reference: 現在你知道如何防疫 , 如何給蚊帳\n",
            "2026-01-05 14:50:10 | INFO | hw5.seq2seq | validation loss:\t4.0679\n",
            "2026-01-05 14:50:10 | INFO | hw5.seq2seq | BLEU = 15.41 49.4/23.8/12.3/6.7 (BP = 0.875 ratio = 0.882 hyp_len = 99404 ref_len = 112709)\n",
            "2026-01-05 14:50:10 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint3.pt\n",
            "2026-01-05 14:50:10 | INFO | hw5.seq2seq | end of epoch 3\n",
            "2026-01-05 14:50:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 14:52:34 | INFO | hw5.seq2seq | training loss: 4.0832           \n",
            "2026-01-05 14:52:34 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 14:52:53 | INFO | hw5.seq2seq | example source: but then you lose control of it .\n",
            "2026-01-05 14:52:53 | INFO | hw5.seq2seq | example hypothesis: 但你失去了 。\n",
            "2026-01-05 14:52:53 | INFO | hw5.seq2seq | example reference: 但之後你就控制不了了 。\n",
            "2026-01-05 14:52:53 | INFO | hw5.seq2seq | validation loss:\t3.8999\n",
            "2026-01-05 14:52:53 | INFO | hw5.seq2seq | BLEU = 15.45 56.9/29.1/15.8/9.0 (BP = 0.702 ratio = 0.739 hyp_len = 83258 ref_len = 112709)\n",
            "2026-01-05 14:52:53 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint4.pt\n",
            "2026-01-05 14:52:53 | INFO | hw5.seq2seq | end of epoch 4\n",
            "2026-01-05 14:52:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 14:55:16 | INFO | hw5.seq2seq | training loss: 3.8899           \n",
            "2026-01-05 14:55:16 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 14:55:39 | INFO | hw5.seq2seq | example source: and hansen is hard over for nuclear power , as are most climatologists who are engaging this issue seriously .\n",
            "2026-01-05 14:55:39 | INFO | hw5.seq2seq | example hypothesis: 漢斯對核電力很困難 , 如同大部分的攀登學家 , 他們都很嚴肅地參與這個議題 。\n",
            "2026-01-05 14:55:39 | INFO | hw5.seq2seq | example reference: 漢森致力研究核能就像許多氣候學家正專注這個問題\n",
            "2026-01-05 14:55:39 | INFO | hw5.seq2seq | validation loss:\t3.6944\n",
            "2026-01-05 14:55:39 | INFO | hw5.seq2seq | BLEU = 20.15 52.3/27.2/14.9/8.7 (BP = 0.971 ratio = 0.972 hyp_len = 109529 ref_len = 112709)\n",
            "2026-01-05 14:55:40 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint5.pt\n",
            "2026-01-05 14:55:40 | INFO | hw5.seq2seq | end of epoch 5\n",
            "2026-01-05 14:55:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 14:58:03 | INFO | hw5.seq2seq | training loss: 3.7358           \n",
            "2026-01-05 14:58:03 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 14:58:23 | INFO | hw5.seq2seq | example source: tattooing machines used today insert tiny needles , loaded with dye , into the skin at a frequency of 50 to 3 , 000 times per minute .\n",
            "2026-01-05 14:58:23 | INFO | hw5.seq2seq | example hypothesis: 現今 , 刺青機器使用的小針頭 , 裝上牙齒 , 每分鐘有五到三萬次的皮膚 。\n",
            "2026-01-05 14:58:23 | INFO | hw5.seq2seq | example reference: 現代用的紋身機細微針頭吸取墨水刺進皮膚以每分鐘刺50到3000下的頻率\n",
            "2026-01-05 14:58:23 | INFO | hw5.seq2seq | validation loss:\t3.5671\n",
            "2026-01-05 14:58:23 | INFO | hw5.seq2seq | BLEU = 21.09 57.3/31.0/17.6/10.5 (BP = 0.882 ratio = 0.889 hyp_len = 100179 ref_len = 112709)\n",
            "2026-01-05 14:58:24 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint6.pt\n",
            "2026-01-05 14:58:24 | INFO | hw5.seq2seq | end of epoch 6\n",
            "2026-01-05 14:58:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:00:47 | INFO | hw5.seq2seq | training loss: 3.5807           \n",
            "2026-01-05 15:00:47 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:01:09 | INFO | hw5.seq2seq | example source: i dream about this future every single day .\n",
            "2026-01-05 15:01:09 | INFO | hw5.seq2seq | example hypothesis: 我每天都夢想著這個未來 。\n",
            "2026-01-05 15:01:09 | INFO | hw5.seq2seq | example reference: 我每天都在夢想這個未來 。\n",
            "2026-01-05 15:01:09 | INFO | hw5.seq2seq | validation loss:\t3.4689\n",
            "2026-01-05 15:01:09 | INFO | hw5.seq2seq | BLEU = 22.05 57.8/31.7/18.2/10.9 (BP = 0.897 ratio = 0.902 hyp_len = 101701 ref_len = 112709)\n",
            "2026-01-05 15:01:09 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint7.pt\n",
            "2026-01-05 15:01:10 | INFO | hw5.seq2seq | end of epoch 7\n",
            "2026-01-05 15:01:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:03:33 | INFO | hw5.seq2seq | training loss: 3.4707           \n",
            "2026-01-05 15:03:33 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:03:54 | INFO | hw5.seq2seq | example source: actually , the footprint on the ground for wind is by far the smallest of any energy source in the world .\n",
            "2026-01-05 15:03:54 | INFO | hw5.seq2seq | example hypothesis: 事實上 , 風力發電的足跡是世界上最小的能源來源 。\n",
            "2026-01-05 15:03:54 | INFO | hw5.seq2seq | example reference: 事實上 , 風力發電場佔地面積是低於全世界任何一種發電方式\n",
            "2026-01-05 15:03:54 | INFO | hw5.seq2seq | validation loss:\t3.3960\n",
            "2026-01-05 15:03:54 | INFO | hw5.seq2seq | BLEU = 23.19 58.4/32.4/18.9/11.6 (BP = 0.914 ratio = 0.918 hyp_len = 103416 ref_len = 112709)\n",
            "2026-01-05 15:03:54 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint8.pt\n",
            "2026-01-05 15:03:54 | INFO | hw5.seq2seq | end of epoch 8\n",
            "2026-01-05 15:03:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:06:18 | INFO | hw5.seq2seq | training loss: 3.3808           \n",
            "2026-01-05 15:06:18 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:06:38 | INFO | hw5.seq2seq | example source: these are why people pray .\n",
            "2026-01-05 15:06:38 | INFO | hw5.seq2seq | example hypothesis: 這就是為什麼人們會禱告 。\n",
            "2026-01-05 15:06:38 | INFO | hw5.seq2seq | example reference: 這是人們禱告的原因\n",
            "2026-01-05 15:06:38 | INFO | hw5.seq2seq | validation loss:\t3.3442\n",
            "2026-01-05 15:06:38 | INFO | hw5.seq2seq | BLEU = 24.18 58.0/32.5/19.0/11.7 (BP = 0.951 ratio = 0.952 hyp_len = 107342 ref_len = 112709)\n",
            "2026-01-05 15:06:38 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint9.pt\n",
            "2026-01-05 15:06:39 | INFO | hw5.seq2seq | end of epoch 9\n",
            "2026-01-05 15:06:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:09:04 | INFO | hw5.seq2seq | training loss: 3.3135            \n",
            "2026-01-05 15:09:04 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:09:25 | INFO | hw5.seq2seq | example source: and i gathered together , honestly , the world's leading experts in every possible way .\n",
            "2026-01-05 15:09:25 | INFO | hw5.seq2seq | example hypothesis: 老實說 , 我聚集在一起 , 世界各地的頂尖專家 。\n",
            "2026-01-05 15:09:25 | INFO | hw5.seq2seq | example reference: 說真的 , 我把世界級的專家們用各種方法聚集在一起幫助我\n",
            "2026-01-05 15:09:25 | INFO | hw5.seq2seq | validation loss:\t3.3126\n",
            "2026-01-05 15:09:25 | INFO | hw5.seq2seq | BLEU = 24.70 58.0/32.6/19.2/11.9 (BP = 0.963 ratio = 0.963 hyp_len = 108577 ref_len = 112709)\n",
            "2026-01-05 15:09:25 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint10.pt\n",
            "2026-01-05 15:09:25 | INFO | hw5.seq2seq | end of epoch 10\n",
            "2026-01-05 15:09:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:11:51 | INFO | hw5.seq2seq | training loss: 3.2595            \n",
            "2026-01-05 15:11:51 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:12:10 | INFO | hw5.seq2seq | example source: we're defined by our environment and our interaction with that environment , by our ecology .\n",
            "2026-01-05 15:12:10 | INFO | hw5.seq2seq | example hypothesis: 我們被環境和我們與生態環境的互動所定義 。\n",
            "2026-01-05 15:12:10 | INFO | hw5.seq2seq | example reference: 我們是被我們的環境以及我們和環境之間的交互作用所定義也就是我們的生態\n",
            "2026-01-05 15:12:10 | INFO | hw5.seq2seq | validation loss:\t3.2829\n",
            "2026-01-05 15:12:10 | INFO | hw5.seq2seq | BLEU = 24.43 59.2/33.3/19.6/12.2 (BP = 0.933 ratio = 0.935 hyp_len = 105356 ref_len = 112709)\n",
            "2026-01-05 15:12:11 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint11.pt\n",
            "2026-01-05 15:12:11 | INFO | hw5.seq2seq | end of epoch 11\n",
            "2026-01-05 15:12:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:14:34 | INFO | hw5.seq2seq | training loss: 3.2112            \n",
            "2026-01-05 15:14:34 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:14:55 | INFO | hw5.seq2seq | example source: now , how can you have a deep tan and have no wrinkles ?\n",
            "2026-01-05 15:14:55 | INFO | hw5.seq2seq | example hypothesis: 那麼 , 你要如何才能有一根深蒂固的水槽 ?\n",
            "2026-01-05 15:14:55 | INFO | hw5.seq2seq | example reference: 怎樣才能有古銅色的皮膚而且沒有<unk>紋 ?\n",
            "2026-01-05 15:14:55 | INFO | hw5.seq2seq | validation loss:\t3.2597\n",
            "2026-01-05 15:14:55 | INFO | hw5.seq2seq | BLEU = 25.08 59.1/33.5/20.0/12.6 (BP = 0.945 ratio = 0.946 hyp_len = 106653 ref_len = 112709)\n",
            "2026-01-05 15:14:55 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint12.pt\n",
            "2026-01-05 15:14:55 | INFO | hw5.seq2seq | end of epoch 12\n",
            "2026-01-05 15:14:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:17:17 | INFO | hw5.seq2seq | training loss: 3.1711            \n",
            "2026-01-05 15:17:17 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:17:39 | INFO | hw5.seq2seq | example source: the second time , derartu tulu grabs her and tries to pull her .\n",
            "2026-01-05 15:17:39 | INFO | hw5.seq2seq | example hypothesis: 第二次 , derartutulu抓住她 , 試著拉她 。\n",
            "2026-01-05 15:17:39 | INFO | hw5.seq2seq | example reference: 而derartutulu再次抓住她 , 試圖拉扯她前進 。\n",
            "2026-01-05 15:17:39 | INFO | hw5.seq2seq | validation loss:\t3.2404\n",
            "2026-01-05 15:17:39 | INFO | hw5.seq2seq | BLEU = 25.14 60.0/34.3/20.5/12.9 (BP = 0.927 ratio = 0.929 hyp_len = 104758 ref_len = 112709)\n",
            "2026-01-05 15:17:39 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint13.pt\n",
            "2026-01-05 15:17:39 | INFO | hw5.seq2seq | end of epoch 13\n",
            "2026-01-05 15:17:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:20:03 | INFO | hw5.seq2seq | training loss: 3.1347            \n",
            "2026-01-05 15:20:03 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:20:23 | INFO | hw5.seq2seq | example source: so what exactly is 'oumuamua ?\n",
            "2026-01-05 15:20:23 | INFO | hw5.seq2seq | example hypothesis: 那麼斥侯星到底是什麼 ?\n",
            "2026-01-05 15:20:23 | INFO | hw5.seq2seq | example reference: 那麼斥侯星究竟是什麼 ?\n",
            "2026-01-05 15:20:23 | INFO | hw5.seq2seq | validation loss:\t3.2295\n",
            "2026-01-05 15:20:23 | INFO | hw5.seq2seq | BLEU = 25.29 60.3/34.5/20.7/13.0 (BP = 0.924 ratio = 0.927 hyp_len = 104488 ref_len = 112709)\n",
            "2026-01-05 15:20:23 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint14.pt\n",
            "2026-01-05 15:20:23 | INFO | hw5.seq2seq | end of epoch 14\n",
            "2026-01-05 15:20:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:22:49 | INFO | hw5.seq2seq | training loss: 3.1052            \n",
            "2026-01-05 15:22:49 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:23:09 | INFO | hw5.seq2seq | example source: i have given the slide show that i gave here two years ago about 2 , 000 times .\n",
            "2026-01-05 15:23:09 | INFO | hw5.seq2seq | example hypothesis: 兩年前我給這張投影片 , 大約兩千次 。\n",
            "2026-01-05 15:23:09 | INFO | hw5.seq2seq | example reference: 兩年前 , 我在這裡講過一場演講 , 後來 , 同樣的內容我大概又講了2000次\n",
            "2026-01-05 15:23:09 | INFO | hw5.seq2seq | validation loss:\t3.2125\n",
            "2026-01-05 15:23:09 | INFO | hw5.seq2seq | BLEU = 25.66 59.5/34.1/20.5/13.0 (BP = 0.947 ratio = 0.948 hyp_len = 106881 ref_len = 112709)\n",
            "2026-01-05 15:23:09 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint15.pt\n",
            "2026-01-05 15:23:10 | INFO | hw5.seq2seq | end of epoch 15\n",
            "2026-01-05 15:23:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:25:34 | INFO | hw5.seq2seq | training loss: 3.0765            \n",
            "2026-01-05 15:25:34 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:25:54 | INFO | hw5.seq2seq | example source: we have made all these discoveries pouring out of laboratories across the world .\n",
            "2026-01-05 15:25:54 | INFO | hw5.seq2seq | example hypothesis: 我們已經把這些發現從世界各地的實驗室中湧出來 。\n",
            "2026-01-05 15:25:54 | INFO | hw5.seq2seq | example reference: 我們讓這些發現走出實驗室到世界各地 。\n",
            "2026-01-05 15:25:54 | INFO | hw5.seq2seq | validation loss:\t3.1949\n",
            "2026-01-05 15:25:54 | INFO | hw5.seq2seq | BLEU = 26.02 59.1/33.9/20.4/12.9 (BP = 0.965 ratio = 0.966 hyp_len = 108870 ref_len = 112709)\n",
            "2026-01-05 15:25:55 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint16.pt\n",
            "2026-01-05 15:25:55 | INFO | hw5.seq2seq | end of epoch 16\n",
            "2026-01-05 15:25:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:28:19 | INFO | hw5.seq2seq | training loss: 3.0494            \n",
            "2026-01-05 15:28:19 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:28:41 | INFO | hw5.seq2seq | example source: now remember kurdistan is landlocked .\n",
            "2026-01-05 15:28:41 | INFO | hw5.seq2seq | example hypothesis: 還記得庫德斯坦被鎖定了 。\n",
            "2026-01-05 15:28:41 | INFO | hw5.seq2seq | example reference: 注意 , 庫德斯坦四周皆是陸地 ,\n",
            "2026-01-05 15:28:41 | INFO | hw5.seq2seq | validation loss:\t3.1904\n",
            "2026-01-05 15:28:41 | INFO | hw5.seq2seq | BLEU = 26.05 59.0/33.8/20.3/12.8 (BP = 0.971 ratio = 0.971 hyp_len = 109436 ref_len = 112709)\n",
            "2026-01-05 15:28:41 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint17.pt\n",
            "2026-01-05 15:28:42 | INFO | hw5.seq2seq | end of epoch 17\n",
            "2026-01-05 15:28:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:31:07 | INFO | hw5.seq2seq | training loss: 3.0259            \n",
            "2026-01-05 15:31:07 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:31:27 | INFO | hw5.seq2seq | example source: there's an eight percent mortality rate with just doing this procedure , and so basically and what do they learn from it ?\n",
            "2026-01-05 15:31:27 | INFO | hw5.seq2seq | example hypothesis: 僅僅做這個程序 , 死亡率有8% , 所以基本上 , 他們從中學到什麼 ?\n",
            "2026-01-05 15:31:27 | INFO | hw5.seq2seq | example reference: 手術的致命率為百分之八 , 大體上 , 他們因此得到什麼 ?\n",
            "2026-01-05 15:31:27 | INFO | hw5.seq2seq | validation loss:\t3.1841\n",
            "2026-01-05 15:31:27 | INFO | hw5.seq2seq | BLEU = 26.12 59.6/34.2/20.6/13.1 (BP = 0.960 ratio = 0.960 hyp_len = 108255 ref_len = 112709)\n",
            "2026-01-05 15:31:28 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint18.pt\n",
            "2026-01-05 15:31:28 | INFO | hw5.seq2seq | end of epoch 18\n",
            "2026-01-05 15:31:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:33:52 | INFO | hw5.seq2seq | training loss: 3.0045            \n",
            "2026-01-05 15:33:52 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:34:13 | INFO | hw5.seq2seq | example source: in record time , a team of architects and construction workers designed and built a viewing platform to ease the frustration and bring people closer .\n",
            "2026-01-05 15:34:13 | INFO | hw5.seq2seq | example hypothesis: 紀錄時間 , 一組建築師和建築師團隊設計並建立了一個觀看平台 , 來緩解挫折 , 讓人們更接近 。\n",
            "2026-01-05 15:34:13 | INFO | hw5.seq2seq | example reference: 一群建築師和建築工人 , 正以破紀錄的速度 , 設計並建造一個觀景平台 , 以解除大家的困擾 , 大家可以更靠近地觀看這個遺址 。\n",
            "2026-01-05 15:34:13 | INFO | hw5.seq2seq | validation loss:\t3.1798\n",
            "2026-01-05 15:34:13 | INFO | hw5.seq2seq | BLEU = 26.38 59.8/34.4/20.8/13.3 (BP = 0.962 ratio = 0.963 hyp_len = 108485 ref_len = 112709)\n",
            "2026-01-05 15:34:13 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint19.pt\n",
            "2026-01-05 15:34:13 | INFO | hw5.seq2seq | end of epoch 19\n",
            "2026-01-05 15:34:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:36:44 | INFO | hw5.seq2seq | training loss: 2.9834            \n",
            "2026-01-05 15:36:44 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:37:10 | INFO | hw5.seq2seq | example source: this is the former trolley terminal that was the depot for passengers traveling over the williamsburg bridge from brooklyn to manhattan , and it was open between 1908 and 1948 , just around the time when my grandparents were living right in the area .\n",
            "2026-01-05 15:37:10 | INFO | hw5.seq2seq | example hypothesis: 這是之前的電車終端 , 旅客橫越威廉斯堡橋 , 從布魯克林到曼哈頓 , 它在1908年和1948年間開放 , 就在我祖父母住的地方 。\n",
            "2026-01-05 15:37:10 | INFO | hw5.seq2seq | example reference: 這地方以前是電車站 , 是乘客從布魯克林區經由威廉斯堡橋到曼哈頓的火車站 , 於1908年到1948年之間使用 。 那時我祖父母剛好住在那個地區 。\n",
            "2026-01-05 15:37:10 | INFO | hw5.seq2seq | validation loss:\t3.1762\n",
            "2026-01-05 15:37:10 | INFO | hw5.seq2seq | BLEU = 26.26 59.9/34.5/20.9/13.3 (BP = 0.954 ratio = 0.955 hyp_len = 107688 ref_len = 112709)\n",
            "2026-01-05 15:37:10 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint20.pt\n",
            "2026-01-05 15:37:10 | INFO | hw5.seq2seq | end of epoch 20\n",
            "2026-01-05 15:37:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:39:40 | INFO | hw5.seq2seq | training loss: 2.9658            \n",
            "2026-01-05 15:39:40 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:40:02 | INFO | hw5.seq2seq | example source: you don't hear about lion attacks , there aren't a lot of lions around .\n",
            "2026-01-05 15:40:02 | INFO | hw5.seq2seq | example hypothesis: 你不會聽說獅子攻擊 , 也不會有很多獅子 。\n",
            "2026-01-05 15:40:02 | INFO | hw5.seq2seq | example reference: 沒聽到獅子攻擊事件 , 就表示附近的獅子不多直到新聞報紙被發明前 , 這種判斷準則是成立的\n",
            "2026-01-05 15:40:02 | INFO | hw5.seq2seq | validation loss:\t3.1696\n",
            "2026-01-05 15:40:02 | INFO | hw5.seq2seq | BLEU = 26.57 59.7/34.5/20.9/13.4 (BP = 0.963 ratio = 0.964 hyp_len = 108596 ref_len = 112709)\n",
            "2026-01-05 15:40:02 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint21.pt\n",
            "2026-01-05 15:40:02 | INFO | hw5.seq2seq | end of epoch 21\n",
            "2026-01-05 15:40:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:42:47 | INFO | hw5.seq2seq | training loss: 2.9486            \n",
            "2026-01-05 15:42:47 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:43:07 | INFO | hw5.seq2seq | example source: strike us down , the music lives , dark , like tar or tobacco or cotton in muddy water .\n",
            "2026-01-05 15:43:07 | INFO | hw5.seq2seq | example hypothesis: 把我們擊倒 , 音樂是活的 , 黑暗的 , 像是焦油或菸草 , 或是泥水中的棉花 。\n",
            "2026-01-05 15:43:07 | INFO | hw5.seq2seq | example reference: 我們倒下 , 音樂永存 。 黑色 , 如焦油與菸絲 , 如棉花浸泡於泥水 。\n",
            "2026-01-05 15:43:07 | INFO | hw5.seq2seq | validation loss:\t3.1698\n",
            "2026-01-05 15:43:07 | INFO | hw5.seq2seq | BLEU = 26.53 60.6/35.2/21.4/13.8 (BP = 0.942 ratio = 0.943 hyp_len = 106339 ref_len = 112709)\n",
            "2026-01-05 15:43:08 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint22.pt\n",
            "2026-01-05 15:43:08 | INFO | hw5.seq2seq | end of epoch 22\n",
            "2026-01-05 15:43:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:45:36 | INFO | hw5.seq2seq | training loss: 2.9337            \n",
            "2026-01-05 15:45:36 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:45:56 | INFO | hw5.seq2seq | example source: sk: think about the things that we need to change .\n",
            "2026-01-05 15:45:56 | INFO | hw5.seq2seq | example hypothesis: 史:想想我們需要改變的東西 。\n",
            "2026-01-05 15:45:56 | INFO | hw5.seq2seq | example reference: 史:想想我們需要改變的事物 。\n",
            "2026-01-05 15:45:56 | INFO | hw5.seq2seq | validation loss:\t3.1619\n",
            "2026-01-05 15:45:56 | INFO | hw5.seq2seq | BLEU = 26.81 59.2/34.1/20.7/13.2 (BP = 0.984 ratio = 0.984 hyp_len = 110949 ref_len = 112709)\n",
            "2026-01-05 15:45:57 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint23.pt\n",
            "2026-01-05 15:45:57 | INFO | hw5.seq2seq | end of epoch 23\n",
            "2026-01-05 15:45:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:48:24 | INFO | hw5.seq2seq | training loss: 2.9181            \n",
            "2026-01-05 15:48:24 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:48:46 | INFO | hw5.seq2seq | example source: you're watching the life cycle of a streptomyces coelicolor .\n",
            "2026-01-05 15:48:46 | INFO | hw5.seq2seq | example hypothesis: 你正在看一個儲存容器冷卻劑的生命週期 。\n",
            "2026-01-05 15:48:46 | INFO | hw5.seq2seq | example reference: 你正在看的是天藍色鏈黴菌的生命週期 。\n",
            "2026-01-05 15:48:46 | INFO | hw5.seq2seq | validation loss:\t3.1546\n",
            "2026-01-05 15:48:46 | INFO | hw5.seq2seq | BLEU = 26.72 59.7/34.5/21.0/13.4 (BP = 0.970 ratio = 0.970 hyp_len = 109326 ref_len = 112709)\n",
            "2026-01-05 15:48:46 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint24.pt\n",
            "2026-01-05 15:48:46 | INFO | hw5.seq2seq | end of epoch 24\n",
            "2026-01-05 15:48:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:51:17 | INFO | hw5.seq2seq | training loss: 2.9036            \n",
            "2026-01-05 15:51:17 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:51:36 | INFO | hw5.seq2seq | example source: and it all begins really with yourself asking this question: what is your longpath ?\n",
            "2026-01-05 15:51:36 | INFO | hw5.seq2seq | example hypothesis: 這一切都是從你開始問這個問題:你的長途是什麼 ?\n",
            "2026-01-05 15:51:36 | INFO | hw5.seq2seq | example reference: 一切始於你問自己這個問題:你的未來的長途是甚麼?\n",
            "2026-01-05 15:51:36 | INFO | hw5.seq2seq | validation loss:\t3.1627\n",
            "2026-01-05 15:51:36 | INFO | hw5.seq2seq | BLEU = 25.99 61.1/35.5/21.5/13.8 (BP = 0.918 ratio = 0.921 hyp_len = 103807 ref_len = 112709)\n",
            "2026-01-05 15:51:37 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint25.pt\n",
            "2026-01-05 15:51:37 | INFO | hw5.seq2seq | end of epoch 25\n",
            "2026-01-05 15:51:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:54:02 | INFO | hw5.seq2seq | training loss: 2.8907            \n",
            "2026-01-05 15:54:02 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:54:23 | INFO | hw5.seq2seq | example source: i took the idea of the wristwatch , and i turned it into a musical score .\n",
            "2026-01-05 15:54:23 | INFO | hw5.seq2seq | example hypothesis: 我把手錶的概念轉化為音樂分數 。\n",
            "2026-01-05 15:54:23 | INFO | hw5.seq2seq | example reference: 我將手錶轉化成樂譜\n",
            "2026-01-05 15:54:23 | INFO | hw5.seq2seq | validation loss:\t3.1590\n",
            "2026-01-05 15:54:23 | INFO | hw5.seq2seq | BLEU = 26.57 60.1/34.8/21.1/13.5 (BP = 0.955 ratio = 0.956 hyp_len = 107758 ref_len = 112709)\n",
            "2026-01-05 15:54:23 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint26.pt\n",
            "2026-01-05 15:54:23 | INFO | hw5.seq2seq | end of epoch 26\n",
            "2026-01-05 15:54:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:56:47 | INFO | hw5.seq2seq | training loss: 2.8777            \n",
            "2026-01-05 15:56:47 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:57:14 | INFO | hw5.seq2seq | example source: and here's the rub: if you're facing old age , and your partner isn't doing that for you in fact , you're having to do that for them then in an alreadyfragile relationship , it can look a bit like you might be better off out of it rather than in it .\n",
            "2026-01-05 15:57:14 | INFO | hw5.seq2seq | example hypothesis: 魯布:如果你面對年紀很老 , 你的另一半並不會為你這麼做 , 事實上 , 你得要為他們建立一個已經很脆弱的關係 , 你可能會覺得你好像比較好 , 而不是在其中 。\n",
            "2026-01-05 15:57:14 | INFO | hw5.seq2seq | example reference: 困難處在這裡:如果你步入老年 , 而你的另一半沒有為你做這些事實上 , 是你得要為他們做這些那麼 , 在已經很脆弱的關係中 , 你可能就會覺得 , 脫離關係會比留在關係中更好些 。\n",
            "2026-01-05 15:57:14 | INFO | hw5.seq2seq | validation loss:\t3.1561\n",
            "2026-01-05 15:57:14 | INFO | hw5.seq2seq | BLEU = 26.41 60.4/35.0/21.3/13.6 (BP = 0.945 ratio = 0.946 hyp_len = 106645 ref_len = 112709)\n",
            "2026-01-05 15:57:15 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint27.pt\n",
            "2026-01-05 15:57:15 | INFO | hw5.seq2seq | end of epoch 27\n",
            "2026-01-05 15:57:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 15:59:39 | INFO | hw5.seq2seq | training loss: 2.8651            \n",
            "2026-01-05 15:59:39 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 15:59:59 | INFO | hw5.seq2seq | example source: yeah . so , with social media and a vibrant , vibrant african diaspora , more and more the world is dancing to our rhythm and talking our pidgin .\n",
            "2026-01-05 15:59:59 | INFO | hw5.seq2seq | example hypothesis: 是的 。 所以 , 透過社交媒體和活力 , 活躍的非洲僑民 , 世界越來越會跳舞 , 跳舞到我們的節奏中 , 說我們的披頭四 。\n",
            "2026-01-05 15:59:59 | INFO | hw5.seq2seq | example reference: 是啊 。 配合社交媒體和離鄉背井的那些充滿生氣的非洲人 , 世界上越來越多人跟著我們的節奏跳舞 , 說我們的洋涇濱語 。\n",
            "2026-01-05 15:59:59 | INFO | hw5.seq2seq | validation loss:\t3.1537\n",
            "2026-01-05 15:59:59 | INFO | hw5.seq2seq | BLEU = 26.59 60.0/34.6/20.9/13.4 (BP = 0.963 ratio = 0.964 hyp_len = 108609 ref_len = 112709)\n",
            "2026-01-05 16:00:00 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint28.pt\n",
            "2026-01-05 16:00:00 | INFO | hw5.seq2seq | end of epoch 28\n",
            "2026-01-05 16:00:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 16:02:26 | INFO | hw5.seq2seq | training loss: 2.8537            \n",
            "2026-01-05 16:02:26 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 16:02:48 | INFO | hw5.seq2seq | example source: this isn't an outlier .\n",
            "2026-01-05 16:02:48 | INFO | hw5.seq2seq | example hypothesis: 這不是外星人 。\n",
            "2026-01-05 16:02:48 | INFO | hw5.seq2seq | example reference: 這並不是個案 。\n",
            "2026-01-05 16:02:48 | INFO | hw5.seq2seq | validation loss:\t3.1520\n",
            "2026-01-05 16:02:48 | INFO | hw5.seq2seq | BLEU = 26.57 59.4/34.3/20.7/13.2 (BP = 0.972 ratio = 0.972 hyp_len = 109581 ref_len = 112709)\n",
            "2026-01-05 16:02:48 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint29.pt\n",
            "2026-01-05 16:02:48 | INFO | hw5.seq2seq | end of epoch 29\n",
            "2026-01-05 16:02:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 794\n",
            "2026-01-05 16:05:11 | INFO | hw5.seq2seq | training loss: 2.8434            \n",
            "2026-01-05 16:05:11 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 16:05:32 | INFO | hw5.seq2seq | example source: instead , it looks like this .\n",
            "2026-01-05 16:05:32 | INFO | hw5.seq2seq | example hypothesis: 相反地 , 它看起來像這樣 。\n",
            "2026-01-05 16:05:32 | INFO | hw5.seq2seq | example reference: 而是這個 。\n",
            "2026-01-05 16:05:32 | INFO | hw5.seq2seq | validation loss:\t3.1518\n",
            "2026-01-05 16:05:32 | INFO | hw5.seq2seq | BLEU = 26.78 59.5/34.4/20.8/13.3 (BP = 0.977 ratio = 0.977 hyp_len = 110101 ref_len = 112709)\n",
            "2026-01-05 16:05:33 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/users/weihao/dl/model_h5/checkpoints/rnn/checkpoint30.pt\n",
            "2026-01-05 16:05:33 | INFO | hw5.seq2seq | end of epoch 30\n"
          ]
        }
      ],
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "print(\"next_epoch_idx =\", epoch_itr.next_epoch_idx)\n",
        "print(\"max_epoch =\", config.max_epoch)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))\n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjRwllxPjtf"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "N70Gc6smPi1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python: can't open file '/home/users/weihao/dl/model_h5/./fairseq/scripts/average_checkpoints.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "checkdir=config.savedir\n",
        "!python ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGMiun8PnZy"
      },
      "source": [
        "## Confirm model weights used to generate submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tvRdivVUPnsU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 16:05:33 | INFO | hw5.seq2seq | no checkpoints found at checkpoints/rnn/avg_last_5_checkpoint.pt!\n",
            "2026-01-05 16:05:33 | INFO | hw5.seq2seq | begin validation\n",
            "2026-01-05 16:05:54 | INFO | hw5.seq2seq | example source: once we manage to remove the bubbles from the ink that we produce , it is time for celebration .\n",
            "2026-01-05 16:05:54 | INFO | hw5.seq2seq | example hypothesis: 一旦我們能把泡泡從墨水中除去 , 該是慶祝的時候了 。\n",
            "2026-01-05 16:05:54 | INFO | hw5.seq2seq | example reference: 一旦我們能夠把泡泡從我們所製造的墨水中除去 , 就該大肆慶祝了 。\n",
            "2026-01-05 16:05:54 | INFO | hw5.seq2seq | validation loss:\t3.1518\n",
            "2026-01-05 16:05:54 | INFO | hw5.seq2seq | BLEU = 26.78 59.5/34.4/20.8/13.3 (BP = 0.977 ratio = 0.977 hyp_len = 110101 ref_len = 112709)\n"
          ]
        }
      ],
      "source": [
        "# checkpoint_last.pt : latest epoch\n",
        "# checkpoint_best.pt : highest validation bleu\n",
        "# avg_last_5_checkpoint.pt: the average of last 5 epochs\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioAIflXpPsxt"
      },
      "source": [
        "## Generate Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "oYMxA8FlPtIq"
      },
      "outputs": [],
      "source": [
        "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):\n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "\n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "\n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "\n",
        "    # sort based on the order before preprocess\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "\n",
        "    with open(outfile, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Le4RFWXxjmm0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 16:05:54 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020/test.en-zh.en\n",
            "2026-01-05 16:05:54 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020/test.en-zh.zh\n",
            "2026-01-05 16:05:54 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 test en-zh 4000 examples\n",
            "prediction: 100%|██████████| 18/18 [00:22<00:00,  1.27s/it]\n"
          ]
        }
      ],
      "source": [
        "generate_prediction(model, task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "wvenyi6BPwnD"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "No active exception to reraise",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
          ]
        }
      ],
      "source": [
        "raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0cJE-wPzaU"
      },
      "source": [
        "# Back-translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7uPJ2CP0sm"
      },
      "source": [
        "## Train a backward translation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGHjg2ZP3sV"
      },
      "source": [
        "1. Switch the source_lang and target_lang in **config**\n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-back\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waTGz29UP6WI"
      },
      "source": [
        "## Generate synthetic data with backward model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIeTsPexP8FL"
      },
      "source": [
        "### Download monolingual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7N4QlsbP8fh"
      },
      "outputs": [],
      "source": [
        "mono_dataset_name = 'mono'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "396saD9-QBPY"
      },
      "outputs": [],
      "source": [
        "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
        "mono_prefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "urls = (\n",
        "    \"https://github.com/figisiwirf/ml2023-hw5-dataset/releases/download/v1.0.1/ted_zh_corpus.deduped.gz\",\n",
        ")\n",
        "file_names = (\n",
        "    'ted_zh_corpus.deduped.gz',\n",
        ")\n",
        "\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = mono_prefix/f\n",
        "    if not path.exists():\n",
        "        !wget {u} -O {path}\n",
        "    else:\n",
        "        print(f'{f} is exist, skip downloading')\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "    elif path.suffix == \".gz\":\n",
        "        !gzip -fkd {path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOVQRHzGQU4-"
      },
      "source": [
        "### TODO: clean corpus\n",
        "\n",
        "1. remove sentences that are too long or too short\n",
        "2. unify punctuation\n",
        "\n",
        "hint: you can use clean_s() defined above to do this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIYmxfUOQSov"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jegH0bvMQVmR"
      },
      "source": [
        "### TODO: Subword Units\n",
        "\n",
        "Use the spm model of the backward model to tokenize the data into subword units\n",
        "\n",
        "hint: spm model is located at DATA/raw-data/\\[dataset\\]/spm\\[vocab_num\\].model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqgR4uUMQZGY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a65glBVXQZiE"
      },
      "source": [
        "### Binarize\n",
        "\n",
        "use fairseq to binarize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b803qA5aQaEu"
      },
      "outputs": [],
      "source": [
        "binpath = Path('./DATA/data-bin', mono_dataset_name)\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = str(mono_prefix/\"mono.tok\") # whatever filepath you get after applying subword tokenization\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smA0JraEQdxz"
      },
      "source": [
        "### TODO: Generate synthetic data with backward model\n",
        "\n",
        "Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "\n",
        "ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "\n",
        "then you can use 'generate_prediction(model, task, split=\"split_name\")' to generate translation prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvaOVHeoQfkB"
      },
      "outputs": [],
      "source": [
        "# Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "# ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.bin ./DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.idx ./DATA/data-bin/ted2020/mono.zh-en.en.idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFEkxPu-Qhlc"
      },
      "outputs": [],
      "source": [
        "# hint: do prediction on split='mono' to create prediction_file\n",
        "# generate_prediction( ... ,split=... ,outfile=... )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn4XeawpQjLk"
      },
      "source": [
        "### TODO: Create new dataset\n",
        "\n",
        "1. Combine the prediction data with monolingual data\n",
        "2. Use the original spm model to tokenize data into Subword Units\n",
        "3. Binarize data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R35JTaTQjkm"
      },
      "outputs": [],
      "source": [
        "# Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.\n",
        "#\n",
        "# hint: tokenize prediction_file with the spm model\n",
        "# spm_model.encode(line, out_type=str)\n",
        "# output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
        "#\n",
        "# hint: use fairseq to binarize these two files again\n",
        "# binpath = Path('./DATA/data-bin/synthetic')\n",
        "# src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "# tgt_dict_file = src_dict_file\n",
        "# monopref = ./DATA/rawdata/mono/mono.tok # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
        "# if binpath.exists():\n",
        "#     print(binpath, \"exists, will not overwrite!\")\n",
        "# else:\n",
        "#     !python -m fairseq_cli.preprocess\\\n",
        "#         --source-lang 'zh'\\\n",
        "#         --target-lang 'en'\\\n",
        "#         --trainpref {monopref}\\\n",
        "#         --destdir {binpath}\\\n",
        "#         --srcdict {src_dict_file}\\\n",
        "#         --tgtdict {tgt_dict_file}\\\n",
        "#         --workers 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSkse1tyQnsR"
      },
      "outputs": [],
      "source": [
        "# create a new dataset from all the files prepared above\n",
        "!cp -r ./DATA/data-bin/ted2020/ ./DATA/data-bin/ted2020_with_mono/\n",
        "\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVdxVGO3QrSs"
      },
      "source": [
        "Created new dataset \"ted2020_with_mono\"\n",
        "\n",
        "1. Change the datadir in **config** (\"./DATA/data-bin/ted2020_with_mono\")\n",
        "2. Switch back the source_lang and target_lang in **config** (\"en\", \"zh\")\n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-bt\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-m3IsoJrhmd"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CZU2beUQtl3"
      },
      "source": [
        "1. <a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).\n",
        "2. <a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).\n",
        "3. <a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., & Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n",
        "4. <a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).\n",
        "5. <a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., & Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).\n",
        "6. <a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., & Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).\n",
        "7. <a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).\n",
        "8. https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n",
        "9. https://ithelp.ithome.com.tw/articles/10233122\n",
        "10. https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "11. https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW05/HW05.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrfm6iLJQ0tS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nKb4u67-sT_Z",
        "n1rwQysTsdJq",
        "59si_C0Wsms7",
        "oOpG4EBRLwe_",
        "6ZlE_1JnMv56",
        "UDAPmxjRNEEL",
        "ce5n4eS7NQNy",
        "rUB9f1WCNgMH",
        "VFJlkOMONsc6",
        "Gt1lX3DRO_yU",
        "BAGMiun8PnZy",
        "JOVQRHzGQU4-",
        "jegH0bvMQVmR",
        "a65glBVXQZiE",
        "smA0JraEQdxz",
        "Jn4XeawpQjLk",
        "z-m3IsoJrhmd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (torch)",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
